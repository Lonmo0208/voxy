package me.cortex.voxy.commonImpl.importers;

import com.mojang.serialization.Codec;
import me.cortex.voxy.common.util.ByteBufferBackedInputStream;
import me.cortex.voxy.common.Logger;
import me.cortex.voxy.common.util.MemoryBuffer;
import me.cortex.voxy.common.util.UnsafeUtil;
import me.cortex.voxy.common.voxelization.VoxelizedSection;
import me.cortex.voxy.common.voxelization.WorldConversionFactory;
import me.cortex.voxy.common.world.WorldEngine;
import me.cortex.voxy.common.thread.ServiceSlice;
import me.cortex.voxy.common.thread.ServiceThreadPool;
import net.minecraft.block.Block;
import net.minecraft.block.BlockState;
import net.minecraft.block.Blocks;
import net.minecraft.nbt.*;
import net.minecraft.network.PacketByteBuf;
import net.minecraft.registry.RegistryKeys;
import net.minecraft.registry.entry.RegistryEntry;
import net.minecraft.util.collection.IndexedIterable;
import net.minecraft.world.World;
import net.minecraft.world.biome.Biome;
import net.minecraft.world.biome.BiomeKeys;
import net.minecraft.world.chunk.ChunkNibbleArray;
import net.minecraft.world.chunk.ChunkStatus;
import net.minecraft.world.chunk.PalettedContainer;
import net.minecraft.world.chunk.ReadableContainer;
import net.minecraft.world.storage.ChunkCompressionFormat;
import org.lwjgl.system.MemoryUtil;

import java.io.*;
import java.nio.ByteOrder;
import java.nio.channels.FileChannel;
import java.nio.file.Path;
import java.nio.file.StandardOpenOption;
import java.util.Arrays;
import java.util.concurrent.ConcurrentLinkedDeque;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.function.Consumer;
import java.util.function.Predicate;

public class WorldImporter {

    public interface UpdateCallback {
        void update(int finished, int outof);
    }

    private final WorldEngine world;
    private final ReadableContainer<RegistryEntry<Biome>> defaultBiomeProvider;
    private final Codec<ReadableContainer<RegistryEntry<Biome>>> biomeCodec;
    private final AtomicInteger estimatedTotalChunks = new AtomicInteger();//Slowly converges to the true value
    private final AtomicInteger totalChunks = new AtomicInteger();
    private final AtomicInteger chunksProcessed = new AtomicInteger();

    private final ConcurrentLinkedDeque<Runnable> jobQueue = new ConcurrentLinkedDeque<>();
    private final ServiceSlice threadPool;

    private volatile boolean isRunning;
    public WorldImporter(WorldEngine worldEngine, World mcWorld, ServiceThreadPool servicePool) {
        this.world = worldEngine;
        this.threadPool = servicePool.createServiceNoCleanup("World importer", 1, ()->()->this.jobQueue.poll().run(), ()->this.world.savingService.getTaskCount() < 4000);

        var biomeRegistry = mcWorld.getRegistryManager().getOrThrow(RegistryKeys.BIOME);
        var defaultBiome = biomeRegistry.getOrThrow(BiomeKeys.PLAINS);
        this.defaultBiomeProvider = new ReadableContainer<>() {
            @Override
            public RegistryEntry<Biome> get(int x, int y, int z) {
                return defaultBiome;
            }

            @Override
            public void forEachValue(Consumer<RegistryEntry<Biome>> action) {

            }

            @Override
            public void writePacket(PacketByteBuf buf) {

            }

            @Override
            public int getPacketSize() {
                return 0;
            }

            @Override
            public boolean hasAny(Predicate<RegistryEntry<Biome>> predicate) {
                return false;
            }

            @Override
            public void count(PalettedContainer.Counter<RegistryEntry<Biome>> counter) {

            }

            @Override
            public PalettedContainer<RegistryEntry<Biome>> copy() {
                return null;
            }

            @Override
            public PalettedContainer<RegistryEntry<Biome>> slice() {
                return null;
            }

            @Override
            public Serialized<RegistryEntry<Biome>> serialize(IndexedIterable<RegistryEntry<Biome>> idList, PalettedContainer.PaletteProvider paletteProvider) {
                return null;
            }
        };

        this.biomeCodec = PalettedContainer.createReadableContainerCodec(
                biomeRegistry.getIndexedEntries(), biomeRegistry.getEntryCodec(), PalettedContainer.PaletteProvider.BIOME, biomeRegistry.getOrThrow(BiomeKeys.PLAINS)
        );
    }


    public void shutdown() {
        this.isRunning = false;
        if (this.worker != null) {
            try {
                this.worker.join();
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }
        }
        this.threadPool.shutdown();
    }

    private volatile Thread worker;
    private UpdateCallback updateCallback;
    public void importWorldAsyncStart(File directory, UpdateCallback updateCallback, Consumer<Integer> onCompletion) {
        this.totalChunks.set(0);
        this.estimatedTotalChunks.set(0);
        this.chunksProcessed.set(0);
        this.updateCallback = updateCallback;
        this.worker = new Thread(() -> {
            this.isRunning = true;
            var files = directory.listFiles();
            if (files == null) {
                onCompletion.accept(0);
            }
            Arrays.sort(files, File::compareTo);
            this.estimatedTotalChunks.addAndGet(files.length*1024);
            for (var file : files) {
                if (!file.isFile()) {
                    continue;
                }
                var name = file.getName();
                var sections = name.split("\\.");
                if (sections.length != 4 || (!sections[0].equals("r")) || (!sections[3].equals("mca"))) {
                    System.err.println("Unknown file: " + name);
                    continue;
                }
                int rx = Integer.parseInt(sections[1]);
                int rz = Integer.parseInt(sections[2]);
                this.estimatedTotalChunks.addAndGet(-1024);
                try {
                    this.importRegionFile(file.toPath(), rx, rz);
                } catch (IOException e) {
                    throw new RuntimeException(e);
                }
                while ((this.totalChunks.get()-this.chunksProcessed.get() > 10_000) && this.isRunning) {
                    try {
                        Thread.sleep(1);
                    } catch (InterruptedException e) {
                        throw new RuntimeException(e);
                    }
                }
                if (!this.isRunning) {
                    onCompletion.accept(this.totalChunks.get());
                    return;
                }
            }
            this.threadPool.blockTillEmpty();
            while (this.chunksProcessed.get() != this.totalChunks.get() && this.isRunning) {
                Thread.yield();
                try {
                    Thread.sleep(10);
                } catch (InterruptedException e) {
                    throw new RuntimeException(e);
                }
            }
            onCompletion.accept(this.totalChunks.get());
            this.worker = null;
        });
        this.worker.setName("World importer");
        this.worker.start();

    }

    public boolean isBusy() {
        return this.worker != null;
    }

    private void importRegionFile(Path file, int x, int z) throws IOException {
        try (var fileStream = FileChannel.open(file, StandardOpenOption.READ)) {
            var fileData = new MemoryBuffer(fileStream.size());
            if (fileStream.read(fileData.asByteBuffer(), 0) < 8192) {
                fileData.free();
                System.err.println("Header of region file invalid");
                return;
            }
            this.importRegionFile(fileData, x, z);
            fileData.free();
        }
    }


    private void importRegionFile(MemoryBuffer regionFile, int x, int z) throws IOException {
        //if (true) return;

        //Find and load all saved chunks
        for (int idx = 0; idx < 1024; idx++) {
            int sectorMeta = Integer.reverseBytes(MemoryUtil.memGetInt(regionFile.address+idx*4));//Assumes little endian
            if (sectorMeta == 0) {
                //Empty chunk
                continue;
            }
            int sectorStart = sectorMeta>>>8;
            int sectorCount = sectorMeta&((1<<8)-1);

            //TODO: create memory copy for each section
            var data = new MemoryBuffer(sectorCount*4096).cpyFrom(regionFile.address+sectorStart*4096L);

            boolean addedToQueue = false;
            {
                int m = Integer.reverseBytes(MemoryUtil.memGetInt(data.address));
                byte b = MemoryUtil.memGetByte(data.address+4L);
                if (m == 0) {
                    System.err.println("Chunk is allocated, but stream is missing");
                } else {
                    int n = m - 1;
                    if ((b & 128) != 0) {
                        if (n != 0) {
                            System.err.println("Chunk has both internal and external streams");
                        }
                        System.err.println("Chunk has external stream which is not supported");
                    } else if (n > data.size-5) {
                        System.err.println("Chunk stream is truncated: expected "+n+" but read " + (data.size-5));
                    } else if (n < 0) {
                        System.err.println("Declared size of chunk is negative");
                    } else {
                        addedToQueue = true;
                        this.jobQueue.add(()-> {
                            if (!this.isRunning) {
                                return;
                            }
                            try {
                                try (var decompressedData = this.decompress(b, new InputStream() {
                                    private long offset = 5;//For the initial 5 offset
                                    @Override
                                    public int read() {
                                        return MemoryUtil.memGetByte(data.address + (this.offset++)) & 0xFF;
                                    }

                                    @Override
                                    public int read(byte[] b, int off, int len) {
                                        len = Math.min(len, this.available());
                                        if (len == 0) {
                                            return -1;
                                        }
                                        UnsafeUtil.memcpy(data.address+this.offset, len, b, off); this.offset+=len;
                                        return len;
                                    }

                                    @Override
                                    public int available() {
                                        return (int) (data.size-this.offset);
                                    }
                                })) {
                                    if (decompressedData == null) {
                                        Logger.error("Error decompressing chunk data");
                                    } else {
                                        var nbt = NbtIo.readCompound(decompressedData);
                                        this.importChunkNBT(nbt, x, z);
                                    }
                                }
                            } catch (Exception e) {
                                throw new RuntimeException(e);
                            } finally {
                                data.free();
                            }
                        });
                        this.totalChunks.incrementAndGet();
                        this.estimatedTotalChunks.incrementAndGet();
                        this.threadPool.execute();
                    }
                }
            }
            if (!addedToQueue) {
                data.free();
            }
        }
    }

    private DataInputStream decompress(byte flags, InputStream stream) throws IOException {
        ChunkCompressionFormat chunkStreamVersion = ChunkCompressionFormat.get(flags);
        if (chunkStreamVersion == null) {
            System.err.println("Chunk has invalid chunk stream version");
            return null;
        } else {
            return new DataInputStream(chunkStreamVersion.wrap(stream));
        }
    }

    private void importChunkNBT(NbtCompound chunk, int regionX, int regionZ) {
        if (!chunk.contains("Status")) {
            //Its not real so decrement the chunk
            this.totalChunks.decrementAndGet();
            return;
        }

        //Dont process non full chunk sections
        if (ChunkStatus.byId(chunk.getString("Status")) != ChunkStatus.FULL) {
            this.totalChunks.decrementAndGet();
            return;
        }

        try {
            int x = chunk.getInt("xPos");
            int z = chunk.getInt("zPos");
            if (x>>5 != regionX || z>>5 != regionZ) {
                Logger.error("Chunk position is not located in correct region, expected: (" + regionX + ", " + regionZ+"), got: " + "(" + (x>>5) + ", " + (z>>5)+"), importing anyway");
            }

            for (var sectionE : chunk.getList("sections", NbtElement.COMPOUND_TYPE)) {
                var section = (NbtCompound) sectionE;
                int y = section.getInt("Y");
                this.importSectionNBT(x, y, z, section);
            }
        } catch (Exception e) {
            System.err.println("Exception importing world chunk:");
            e.printStackTrace();
        }

        this.updateCallback.update(this.chunksProcessed.incrementAndGet(), this.estimatedTotalChunks.get());
    }

    private static final ThreadLocal<VoxelizedSection> SECTION_CACHE = ThreadLocal.withInitial(VoxelizedSection::createEmpty);
    private static final Codec<PalettedContainer<BlockState>> BLOCK_STATE_CODEC = PalettedContainer.createPalettedContainerCodec(Block.STATE_IDS, BlockState.CODEC, PalettedContainer.PaletteProvider.BLOCK_STATE, Blocks.AIR.getDefaultState());
    private void importSectionNBT(int x, int y, int z, NbtCompound section) {
        if (section.getCompound("block_states").isEmpty()) {
            return;
        }

        byte[] blockLightData = section.getByteArray("BlockLight");
        byte[] skyLightData = section.getByteArray("SkyLight");

        ChunkNibbleArray blockLight;
        if (blockLightData.length != 0) {
            blockLight = new ChunkNibbleArray(blockLightData);
        } else {
            blockLight = null;
        }

        ChunkNibbleArray skyLight;
        if (skyLightData.length != 0) {
            skyLight = new ChunkNibbleArray(skyLightData);
        } else {
            skyLight = null;
        }

        var blockStatesRes = BLOCK_STATE_CODEC.parse(NbtOps.INSTANCE, section.getCompound("block_states"));
        if (!blockStatesRes.hasResultOrPartial()) {
            //TODO: if its only partial, it means should try to upgrade the nbt format with datafixerupper probably
            return;
        }
        var blockStates = blockStatesRes.getPartialOrThrow();
        var biomes = this.biomeCodec.parse(NbtOps.INSTANCE, section.getCompound("biomes")).result().orElse(this.defaultBiomeProvider);
        VoxelizedSection csec = WorldConversionFactory.convert(
                SECTION_CACHE.get().setPosition(x, y, z),
                this.world.getMapper(),
                blockStates,
                biomes,
                (bx, by, bz, state) -> {
                    int block = 0;
                    int sky = 0;
                    if (blockLight != null) {
                        block = blockLight.get(bx, by, bz);
                    }
                    if (skyLight != null) {
                        sky = skyLight.get(bx, by, bz);
                    }
                    return (byte) (sky|(block<<4));
                }
        );

        WorldConversionFactory.mipSection(csec, this.world.getMapper());

        this.world.insertUpdate(csec);
    }
}
